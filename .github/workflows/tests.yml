name: Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'requirements.txt'
      - 'pyproject.toml'
      - '.github/workflows/tests.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'requirements.txt'
      - 'pyproject.toml'
  schedule:
    # Run weekly on Sunday at 3 AM
    - cron: '0 3 * * 0'
  workflow_dispatch:  # Allow manual triggering

env:
  PYTHON_VERSION: '3.11'
  POSTGRES_VERSION: '15-alpine'
  REDIS_VERSION: '7-alpine'
  NEO4J_VERSION: '5-enterprise'

jobs:
  # ==========================================================================
  # UNIT TESTS
  # ==========================================================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov pytest-mock httpx
      
      - name: Run unit tests with coverage
        run: |
          pytest tests/ -v \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term \
            -m "not integration and not slow"
      
      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-coverage
          path: |
            htmlcov/
            coverage.xml
          retention-days: 7
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results
          path: |
            test-results.xml
          if-no-files-found: ignore

  # ==========================================================================
  # INTEGRATION TESTS
  # ==========================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: unit-tests  # Run after unit tests pass
    services:
      postgres:
        image: postgres:${{ env.POSTGRES_VERSION }}
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: arf_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:${{ env.REDIS_VERSION }}
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
      
      neo4j:
        image: neo4j:${{ env.NEO4J_VERSION }}
        env:
          NEO4J_AUTH: neo4j/test_password
          NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
        options: >-
          --health-cmd "cypher-shell -u neo4j -p test_password 'RETURN 1'"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5
        ports:
          - 7687:7687
          - 7474:7474
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov pytest-mock httpx
      
      - name: Wait for services to be ready
        run: |
          # Wait for PostgreSQL
          until pg_isready -h localhost -p 5432; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          
          # Wait for Redis
          until redis-cli -h localhost -p 6379 ping | grep -q PONG; do
            echo "Waiting for Redis..."
            sleep 2
          done
          
          # Wait for Neo4j
          until cypher-shell -u neo4j -p test_password -a bolt://localhost:7687 'RETURN 1' 2>/dev/null; do
            echo "Waiting for Neo4j..."
            sleep 5
          done
      
      - name: Run database migrations
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/arf_test
          REDIS_URL: redis://localhost:6379/0
          NEO4J_URL: bolt://localhost:7687
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: test_password
        run: |
          # Run Alembic migrations if available
          if [ -f "alembic.ini" ]; then
            alembic upgrade head
          fi
      
      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/arf_test
          REDIS_URL: redis://localhost:6379/0
          NEO4J_URL: bolt://localhost:7687
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: test_password
          TEST_ENVIRONMENT: 'true'
          JWT_SECRET_KEY: 'test-secret-key-for-tests-only'
        run: |
          pytest tests/ -v \
            --cov=src \
            --cov-append \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term \
            -m "integration"
      
      - name: Upload integration test coverage
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-coverage
          path: |
            htmlcov/
            coverage.xml
          retention-days: 7

  # ==========================================================================
  # CODE QUALITY CHECKS
  # ==========================================================================
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install code quality tools
        run: |
          python -m pip install --upgrade pip
          pip install black isort flake8 mypy
      
      - name: Check code formatting with Black
        run: |
          black --check --diff src/ tests/
      
      - name: Check import sorting with isort
        run: |
          isort --check-only --diff src/ tests/
      
      - name: Lint with flake8
        run: |
          flake8 src/ tests/ --count --show-source --statistics
      
      - name: Type checking with mypy
        run: |
          mypy src/ --ignore-missing-imports

  # ==========================================================================
  # SECURITY SCAN
  # ==========================================================================
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install security scanner
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety
      
      - name: Run Bandit security scanner
        run: |
          bandit -r src/ -f json -o bandit-report.json || true
      
      - name: Run Safety dependency check
        run: |
          safety check -r requirements.txt --json --output safety-report.json || true
      
      - name: Upload security reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  # ==========================================================================
  # PERFORMANCE TESTS (OPTIONAL)
  # ==========================================================================
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [unit-tests, integration-tests]
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'  # Run only on schedule or manually
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-benchmark
      
      - name: Run performance/benchmark tests
        run: |
          pytest tests/ -v -m "slow" --benchmark-only || echo "No benchmark tests found"
      
      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            .benchmarks/
          retention-days: 30

  # ==========================================================================
  # TEST SUMMARY AND REPORTING
  # ==========================================================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, code-quality, security-scan]
    if: always()
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Download coverage artifacts
        uses: actions/download-artifact@v4
        with:
          name: unit-test-coverage
          path: coverage-reports/unit
      
      - name: Download coverage artifacts (integration)
        uses: actions/download-artifact@v4
        with:
          name: integration-test-coverage
          path: coverage-reports/integration
      
      - name: Generate combined coverage report
        run: |
          # Combine coverage reports if they exist
          if [ -f "coverage-reports/unit/coverage.xml" ] && [ -f "coverage-reports/integration/coverage.xml" ]; then
            echo "Combining coverage reports..."
            # This would use coverage combine in a real setup
            echo "Coverage reports available for review"
          elif [ -f "coverage-reports/unit/coverage.xml" ]; then
            echo "Using unit test coverage report"
          elif [ -f "coverage-reports/integration/coverage.xml" ]; then
            echo "Using integration test coverage report"
          else
            echo "No coverage reports found"
          fi
      
      - name: Generate test summary
        run: |
          echo "# Test Summary Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ“Š Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Unit Tests Status
          echo "### ðŸ§ª Unit Tests" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.unit-tests.result }}" == "success" ]]; then
            echo "âœ… **PASSED** - All unit tests passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **FAILED** - Unit tests failed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Integration Tests Status
          echo "### ðŸ”— Integration Tests" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.integration-tests.result }}" == "success" ]]; then
            echo "âœ… **PASSED** - All integration tests passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **FAILED** - Integration tests failed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Code Quality Status
          echo "### ðŸ“ Code Quality" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.code-quality.result }}" == "success" ]]; then
            echo "âœ… **PASSED** - Code quality checks passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **FAILED** - Code quality checks failed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Security Scan Status
          echo "### ðŸ”’ Security Scan" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.security-scan.result }}" == "success" ]]; then
            echo "âœ… **PASSED** - Security scan completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **ISSUES FOUND** - Security scan found issues" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Overall Status
          echo "## ðŸŽ¯ Overall Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.unit-tests.result }}" == "success" ]] && \
             [[ "${{ needs.integration-tests.result }}" == "success" ]] && \
             [[ "${{ needs.code-quality.result }}" == "success" ]]; then
            echo "### ðŸŽ‰ **ALL CHECKS PASSED**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The ARF API is ready for deployment!" >> $GITHUB_STEP_SUMMARY
          else
            echo "### âš ï¸ **SOME CHECKS FAILED**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Please review the failed checks before deployment." >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Next Steps
          echo "## ðŸ“‹ Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "1. Review test results in the artifacts" >> $GITHUB_STEP_SUMMARY
          echo "2. Check coverage reports for uncovered code" >> $GITHUB_STEP_SUMMARY
          echo "3. Address any code quality or security issues" >> $GITHUB_STEP_SUMMARY
          echo "4. Merge to main when all checks pass" >> $GITHUB_STEP_SUMMARY
          echo "5. Docker image will be built and pushed automatically" >> $GITHUB_STEP_SUMMARY
      
      - name: Upload final coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: final-coverage-report
          path: coverage-reports/
          retention-days: 30

  # ==========================================================================
  # DEPLOYMENT GATE
  # ==========================================================================
  deployment-gate:
    name: Deployment Gate
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Check if deployment can proceed
        run: |
          echo "# Deployment Readiness Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.unit-tests.result }}" == "success" ]] && \
             [[ "${{ needs.integration-tests.result }}" == "success" ]] && \
             [[ "${{ needs.code-quality.result }}" == "success" ]]; then
            echo "âœ… **DEPLOYMENT APPROVED**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All quality gates passed. The Docker build workflow will now run." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Next:" >> $GITHUB_STEP_SUMMARY
            echo "1. Docker image will be built and pushed to GitHub Packages" >> $GITHUB_STEP_SUMMARY
            echo "2. Deployment to staging/production can proceed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **DEPLOYMENT BLOCKED**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "One or more quality checks failed. Fix the issues before deployment." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Failed checks:" >> $GITHUB_STEP_SUMMARY
            [[ "${{ needs.unit-tests.result }}" != "success" ]] && echo "- Unit Tests" >> $GITHUB_STEP_SUMMARY
            [[ "${{ needs.integration-tests.result }}" != "success" ]] && echo "- Integration Tests" >> $GITHUB_STEP_SUMMARY
            [[ "${{ needs.code-quality.result }}" != "success" ]] && echo "- Code Quality" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
